---
title: "BMI 714 Project"
author: "Harry Li, Stephanie Chen"
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: true
    toc_depth: 3
    toc_float: true
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::knit_engines$set(txt = function(options) {
  code <- paste(options$code, collapse = "\n")
  knitr::engine_output(options, code, NULL)
})
```

## libraries

```{r, echo = FALSE}
library(ggplot2)
library(ggrepel)
library(ggpubr)
library(glmnet)
library(data.table)
library(tidyverse)
library(reshape2)
library(pheatmap)
library(RColorBrewer)
library(caret)
library(pROC)

set.seed(714)
plot.out.dir <- file.path(getwd(), "plots") 
colset <- colorRampPalette(rev(brewer.pal(n=7, name = "RdYlBu")))(100)
```

## EDA

This includes NA handling and correlation check between selected lab variables.

```{r}
var_lib <- read.csv('BMI714_NHANES_VariableDictionary.csv')
nhanes_2020 <- as.data.frame(fread("BMI714_NHANES2020_Data.csv", header = T, sep = ","))
# nhanes_2020 <- read.csv("BMI714_NHANES2020_Data.csv")

# the meta groups that exist within variables, I'm interested in laboratory
table(var_lib$Component)
```

### Dealing with NAs

```{r}
## first subset main dataframe to just lab variables
lab_var <- var_lib%>%
  filter(Component == "Laboratory")%>%
  pull(BMI_714_Variable_Name)

nhanes_lab <- nhanes_2020%>%
  select(all_of(lab_var))
#head(nhanes_lab)
```

Seems like there's a lot of NAs, let's see how bad it is

```{r}
# there exists no patient that did all 438 lab tests, so let's find a subset of lab vars for which we have a good size of patients 
# drop_na(nhanes_lab)

# first let's see how many NA's each variable has
n_patients <- nrow(nhanes_lab)


# I changed from using read.csv to fread for importing nhanes and the code below is not working can you help?
na_prop_list <- c()
for (i in 1:ncol(nhanes_lab)) {
  na_prop_list[i] <- sum(is.na(nhanes_lab[,i]))/n_patients
}
names(na_prop_list) <- colnames(nhanes_lab)

hist(na_prop_list)
# so a majority of the lab vars contain ~0.8 NA's

# let's set 0.5 as an NA threshold for subsetting lab variables, 
# with the hope that the leftover lab vars will yield a non-zero patient cohort
low_na_vars <- names(na_prop_list[na_prop_list<0.5])
length(low_na_vars)
nrow(nhanes_lab%>%select(all_of(low_na_vars))%>%drop_na())
```

We saw that each of the lab variables have differing rate of NAs. 

  *sanity check*: if a variable A has 0.5 NA rate, then that means half of the total 15560 patients will have an NA for that variable. Our goal is to find a subset of patients and a set of variables for which there exists no NAs. If we introduce some variable B also with 0.5 NA rate and we find the patients that have no NA for either variable, we should expect a result size of less than half of 15560 because it's not necessarily true that everyone who tested for var A also tested for var B (we can sure hope).
  
Thus, we have that 1) the lower the NA rate threshold, the less resulting variable subset, 2) the less resuling variable subset, the more patients will be leftover at the end. (consider the base case where only the lab var with NA rate of 0.00001 is left, which means almost all 15560 patients will be selected)

If we look at only the variables with at most 0.5 NA rate, we end up with ~5000 patients. Let's see if we can do better, our goal is to maximize the size of subsetted variables while having the most patients we can work with. 

```{r}
# set threshold value from 0.01 to 0.50, expect more consensus patient as threshold decrease
consensus_patient_n <- c()
filtered_var_n <- c()
i <- 1
for (t in seq(0.01, 0.5, by = 0.01)) {
  low_na_vars <- names(na_prop_list[na_prop_list<t])
  filtered_var_n[i] <- length(low_na_vars)
  consensus_patient_n[i] <- nrow(nhanes_lab%>%select(low_na_vars)%>%drop_na())
  i <- i + 1
}
names(consensus_patient_n) <- seq(0.01, 0.5, by = 0.01)
names(filtered_var_n) <- seq(0.01, 0.5, by = 0.01)

# aggregate results in dataframe for plotting
results_df <- data.frame(
  Threshold = seq(0.01, 0.5, by = 0.01),
  Consensus_Patients = consensus_patient_n,
  Filtered_Variables = filtered_var_n
)%>%filter(Filtered_Variables > 0)


# the lower the threshold the more patients
ggplot(results_df, aes(x = Threshold, y = Consensus_Patients)) +
  geom_line() +
  geom_point() +
  scale_x_reverse() + 
  labs(
    title = "Consensus Patients vs NA Threshold",
    x = "NA Threshold (Reversed)",
    y = "Number of Consensus Patients"
  ) +
  theme_classic()

# the lower the threshold the less variables
ggplot(results_df, aes(x = Threshold, y = Filtered_Variables)) +
  geom_line() +
  geom_point() +
  scale_x_reverse() + 
  labs(
    title = "Filtered Variables vs NA Threshold",
    x = "NA Threshold (Reversed)",
    y = "Number of Filtered Variables"
  ) +
  theme_classic()

# post hoc
label_point <- results_df[results_df$Threshold == 0.31, ]

min(results_df$Filtered_Variables)

# the more variable the less consensus patients 
# pdf(paste0(plot.out.dir, "/na_handling.pdf"), width = 5, height = 4)
ggplot(results_df, aes(x = Filtered_Variables, y = Consensus_Patients)) +
  geom_line() +
  geom_point() +
  geom_text_repel(
    data = label_point,
    aes(label = paste("NA rate =", Threshold, "\n Filtered Variables # =", Filtered_Variables, "\n Patient # =", Consensus_Patients)),
    nudge_x = 30,  # Nudges the label to the right
    nudge_y = 2000, # Nudges the label upwards
    segment.color = "blue", # Line color
    segment.size = 0.5,     # Line thickness
    color = "blue",         # Label text color
    box.padding = 0.5,      # Space around the text
    point.padding = 0.5     # Space around the point
  ) +
  labs(
    title = "Figure 1: Number of Consensus \nPatients vs Number of Filtered Variables",
    x = "Number of Filtered Variables",
    y = "Number of Consensus Patients"
  ) +
  theme_classic()
# dev.off()
```

By elbow rule, we pick NA rate to be 0.31, and continue with the leftover data subset.

### How do the selected lab variables correlate with one another?

We want to see what variables seem to group together, but also look at whether or not we see at least some correlation betwene our hypothesis group of variables (PBCD) and other groups of variables.

```{r}
selected_vars <- names(na_prop_list[na_prop_list<0.31])
nhane_sub <- nhanes_lab%>%select(selected_vars)%>%drop_na()
# summary(nhane_sub)

# what are these lab values actually corresponding to?
table(var_lib%>%filter(BMI_714_Variable_Name %in% colnames(nhane_sub))%>%pull(Data_File_Description))

# large scale correlation check by heatmap
## but first drop two columns that have 0 standard deviation -- the values were probably all 0 anyways
pdf(paste0(plot.out.dir, "/correlation_subsetted_labvar.pdf"), width = 25, height = 25)
nhane_sub <- nhane_sub[, apply(nhane_sub, 2, sd) != 0]
cor_matrix <- cor(nhane_sub, use = "complete.obs", method = "pearson")
sub_anno <- var_lib%>%
  filter(BMI_714_Variable_Name %in% colnames(nhane_sub))%>%
  select(BMI_714_Variable_Name, Data_File_Description)
cor_matrix_ordered <- cor_matrix[,sub_anno$BMI_714_Variable_Name]
p <- pheatmap(cor_matrix_ordered, 
         color = colorRampPalette(brewer.pal(n = 11, name = "Spectral"))(50),
         display_numbers = F, cluster_rows = F, cluster_cols = F)
print(p)
dev.off()
```
Seems like variables within "Complete Blood Count with 5-Part Differential in Whole Blood" and "Cotinine and Hydroxycotinine - Serum" have some strong covariance within the group. Across groups, we see 1) some correlation between some CBC tests and PBCD tests, 2) slight correlation between some PBCD items and HEPB, HEPC, HEPE measurements, 3) notable correlation between blood mercury (P_PBCD_LBXTHG, P_PBCD_LBDTHGSI, P_PBCD_LBDTHGLC) and mercury level in the IHGEM tests...(yeah no sh*t), 4) slight correlation between blood selenium, blood manganese levels and hemoglobin/red cell/platlet measurements

With that, we focus on observation 2), and aim to construct a regression model that can best predict Hepatitis B core antibody (P_HEPBD_LBXHBC) from "Lead, Cadmium, Total Mercury, Selenium, & Manganese - Blood (PBCD)" family of tests. Upon further inspection, we see that there is actually redundancy within the PBCD tests. For example, the three variables P_PBCD_LBXTHG, P_PBCD_LBDTHGSI, and P_PBCD_LBDTHGLC all measure blood mercury, the first in ug/L, second in nmol/L, and the third a code that defines whether or not the level is above some threshold level. We decide to keep only the variables that uses nmol/L for the 5 heavy metal tests because it's the conventional unit used in research papers. To increase the scope of the variables arrive at 8 variables, we additionally include hematocrit (P_CBC_LBXHCT), mean cell volume (P_CBC_LBXMCVSI), and hemoglobin (P_CBC_LBXHGB). 

```{r}
# creating the dataset with the selected variables
# hep_vars <- intersect(var_lib$BMI_714_Variable_Name[grep("P_HEP", var_lib$Data_File_Name)], colnames(nhane_sub))
hep_vars <- "P_HEPBD_LBXHBC"
analysis_ind_var_vec <- c("P_PBCD_LBDBPBSI", "P_PBCD_LBDBCDSI", "P_PBCD_LBDTHGSI", "P_PBCD_LBDBSESI", "P_PBCD_LBDBMNSI", "P_CBC_LBXHCT", "P_CBC_LBXMCVSI", "P_CBC_LBXHGB")
analysis_var_ref <- var_lib%>%filter(BMI_714_Variable_Name %in% c(hep_vars, analysis_ind_var_vec))
# subset the data to only include the variables of interest
nhane_selected <- nhane_sub%>%
  select(all_of(c(hep_vars, analysis_ind_var_vec)))

nhane_selected$P_HEPBD_LBXHBC <- as.factor(ifelse(nhane_selected$P_HEPBD_LBXHBC == 1, 0, 1))
nhane_selected

# table(nhane_selected$P_HEPBD_LBXHBC)
```

### Test-train split

```{r}
# since we have so many observations we can do 0.7 train prop (i'm so happy)
train_prop <- 0.7

n <- nrow(nhane_selected)

# train_indices <- sample(1:n, size = floor(train_prop * n))
# using this protocol to ensure equal ratio of case/control in train/test
train_indices <- createDataPartition(nhane_selected$P_HEPBD_LBXHBC, p = 0.7, list = FALSE)

nhane_train <- nhane_selected[train_indices, ]   # Training set
nhane_test <- nhane_selected[-train_indices, ]  # Test set

# Check the sizes
cat("Training set size:", nrow(nhane_train), "\n")
cat("Test set size:", nrow(nhane_test), "\n")

cat("Class proportions in training set:\n")
print(prop.table(table(nhane_train$P_HEPBD_LBXHBC)))

cat("Class proportions in test set:\n")
print(prop.table(table(nhane_test$P_HEPBD_LBXHBC)))
```


## Model Building

### Logistic Regression

We have 8 continuous independent variables and 1 binary dependent variable. We will use logistic regression to predict the probability of having Hepatitis B core antibody based on the 8 blood test results. I am interested in investigating the performance and analyzing different approaches to feature selection. 

#### Approach 1: Backward Elimination with $\\beta_1$ significance level of 0.05

```{r}
# fit a full model
fit_full <- glm(P_HEPBD_LBXHBC ~ ., data = nhane_train, family = binomial)
#summary(fit_full)   #P_PBCD_LBDBMNSI is not significant

fit1_1 <- update(fit_full, . ~ . - P_PBCD_LBDBMNSI)
summary(fit1_1)

fit1_2 <- update(fit1_1, . ~ . - P_PBCD_LBDBSESI)
summary(fit1_2)
# all variables are p<0.05, we are done now
```

#### Approach 2: Backward Elimination with AIC

```{r}
fit_null <- glm(P_HEPBD_LBXHBC ~ 1, data = nhane_train, family = binomial)
fit_bk_aic <- step(fit_full, scope = list(lower = fit_null, upper = fit_full), data = nhane_train, direction = "backward", trace = F, k = 2)
summary(fit_bk_aic)
```

#### Approach 3: Iterative Evaluation

Method taken from Aparna, pseudo code below:

Let the set of predictor variables be $V$, 
Construct a null model $M_0$
Let $M_curr$ be $M_0$
while $V$ is not empty
  for each $v \in V$
    fit a univariate model $M_{v}$ with $v$
    run F test to compare $M_{v}$ to $M_{curr}$
    record $v$ and the p value of the F test ($p_v$) for $M_v$ in a table
    
  if all $p_v$ > 0.05, stop
  else
    identify $v$ with $min(p_v)$
    $M_curr$=$M_v$
    $V$ = $V$ - $v$

```{r}
# initialize model and variable set
curr_ind_var_set <- analysis_ind_var_vec
fit_iter <- fit_null
while (length(curr_ind_var_set) > 0) {
  # initiatlize var stat table
  var_sig <- data.frame()
  for (ind_var in curr_ind_var_set) {
    # create a formula with +1 variable
    formula_in <- as.formula(paste(paste(deparse(formula(fit_iter)), collapse = ""), "+", ind_var))
    
    # fit the model
    fit <- glm(formula_in, data = nhane_train, family = binomial)
    
    # pulls out p value of this F-test
    curr_p <- anova(fit_iter, fit)[[5]][2]
    
    # store the variable, p value, and AIC
    var_sig <- rbind(var_sig, data.frame(var = ind_var, p_value = curr_p))
  }
  
  if (all(var_sig$p_value > 0.05)) {
    fit_out <- fit_iter
    cat("Finished iterative variable selection, check fit_out for the final model")
    break
  }
  
  # best var by p value
  best_var_p <- var_sig[var_sig$p_value == min(var_sig$p_value),]$var
  fit_iter <- update(fit_iter, as.formula(paste(". ~ . +", best_var_p)))
  curr_ind_var_set <- curr_ind_var_set[curr_ind_var_set != best_var_p]
  
  cat(paste(best_var_p, "added to the model\n"))
}

summary(fit_out)
```
Now that we have the three models, let's compare

```{r}
#Backward Elimination with $\\beta_1$ significance level of 0.05
summary(fit1_2)
#Backward elimination with AIC
summary(fit_bk_aic)
#Iterative evaluation
summary(fit_out)
```


```{r}
#run anova to determine if including P_PBCD_LBDBSESI leads to 
anova(fit_out, fit_bk_aic)
AIC(fit_out)
AIC(fit_bk_aic)
```

Given that the p-value is > 0.05, this means that adding variable P_PBCD_LBDBSESI to the model does not explain significantly more of the unexplained variation compared to the model generated by iterative evaluation. We ran AIC between the two potential models and found that the difference in AIC score was not >=5 points, so we are not able to conclude that the model improves enough to warrant increasing its complexity. Based on these two test metrics, we decide to utilize the model with less variables for ridge regularization, which is fit_out.

```{r}
# Ridge regularization because want to retain all covariates

#remove the variables that were dropped during variable selection
nhane_train_ridge <- nhane_train%>%
  select(-c(P_PBCD_LBDBSESI, P_PBCD_LBDBMNSI))
nhane_test_ridge <- nhane_test%>%
  select(-c(P_PBCD_LBDBSESI, P_PBCD_LBDBMNSI))

nhanes_matrix <- apply(nhane_train_ridge, 2, as.numeric)

#set alpha to 0 for ridge
nhanes_model_ridge <- cv.glmnet(nhanes_matrix[,!colnames(nhanes_matrix) %in% "P_HEPBD_LBXHBC"], nhanes_matrix[,"P_HEPBD_LBXHBC"], nfolds = 10, alpha = 0)
best_lambda <- nhanes_model_ridge$lambda.min
nhanes_final <- glmnet(nhanes_matrix[,!colnames(nhanes_matrix) %in% "P_HEPBD_LBXHBC"], nhanes_matrix[,"P_HEPBD_LBXHBC"], alpha = 0, lambda=best_lambda)
ridge_coefficients<- coef(nhanes_final)
ridge_coefficients_vector <- as.vector(ridge_coefficients)
names(ridge_coefficients_vector) <- rownames(ridge_coefficients)
```

We now compare the coefficients from the ridge regularization to the coefficients of fit_out.

```{r}
comparison_df <- data.frame( 
  Ridge = ridge_coefficients_vector,  
  Standard = fit_out$coefficients[names(ridge_coefficients_vector)] 
)
comparison_df
```

```{r}
# model eval with regular logistic regression
predicted_probs <- predict(fit_out, newdata = nhane_test, type = "response")
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
# table(predicted_classes)
actual_classes <- nhane_test[, "P_HEPBD_LBXHBC"]
# table(actual_classes)
confusion_matrix_logi <- table(Predicted = predicted_classes, Actual = actual_classes)
roc_logi <- roc(actual_classes, predicted_probs, plot = T, print.auc = T, main = "ROC Curve for Logistic Regression")

# model eval with ridge
predicted_probs <- predict(nhanes_final, newx = as.matrix(nhane_test_ridge[, !colnames(nhane_test_ridge) %in% "P_HEPBD_LBXHBC"]), s = "lambda.min", type = "response")
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
# table(predicted_classes)
actual_classes <- nhane_test_ridge[, "P_HEPBD_LBXHBC"]
# table(actual_classes)
confusion_matrix_ridge <- table(Predicted = predicted_classes, Actual = actual_classes)
roc_logi_ridge <- roc(actual_classes, predicted_probs, plot = T, print.auc = T, main = "ROC Curve for Ridge Regression")
```
We now obtain the overview of metrics for the two models.

```{r}
# define a function to get metrics from confusion matrix
calculate_metrics <- function(confusion_matrix) {
  TN <- confusion_matrix[1, 1]
  FP <- confusion_matrix[1, 2]
  FN <- confusion_matrix[2, 1]
  TP <- confusion_matrix[2, 2]
  sensitivity <- TP / (TP + FN)
  specificity <- TN / (TN + FP)

  # return a quick list of metrics
  return(list(
    Sensitivity = sensitivity,
    Specificity = specificity,
    False_Negatives = FN,
    False_Positives = FP
  ))
}

```

```{r}
# perform the functinos on the confusion matrix
metrics_logistic <- calculate_metrics(confusion_matrix_logi)
metrics_ridge <- calculate_metrics(confusion_matrix_ridge)

# make a df
evaluation_df <- data.frame(
  Model = c("Logistic Regression", "Ridge Regression"),
  Sensitivity = c(metrics_logistic$Sensitivity, metrics_ridge$Sensitivity),
  Specificity = c(metrics_logistic$Specificity, metrics_ridge$Specificity),
  False_Negatives = c(metrics_logistic$False_Negatives, metrics_ridge$False_Negatives),
  False_Positives = c(metrics_logistic$False_Positives, metrics_ridge$False_Positives)
)

evaluation_df
```






## 