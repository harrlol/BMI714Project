---
title: "BMI 714 Project"
author: "Harry Li, Stephanie Chen"
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: true
    toc_depth: 3
    toc_float: true
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::knit_engines$set(txt = function(options) {
  code <- paste(options$code, collapse = "\n")
  knitr::engine_output(options, code, NULL)
})
```

## libraries

```{r, echo = FALSE}
library(ggplot2)
library(ggrepel)
library(ggpubr)
library(glmnet)
library(data.table)
library(tidyverse)
library(reshape2)
library(boot)
library(pheatmap)
library(RColorBrewer)
library(caret)
library(pROC)
library(progress)
library(rsq)

plot.out.dir <- file.path(getwd(), "plots") 
colset <- colorRampPalette(rev(brewer.pal(n=7, name = "RdYlBu")))(100)
```

## EDA

This includes NA handling and correlation check between selected lab variables.

```{r}
var_lib <- read.csv('BMI714_NHANES_VariableDictionary.csv')
nhanes_2020 <- as.data.frame(fread("BMI714_NHANES2020_Data.csv", header = T, sep = ","))
# nhanes_2020 <- read.csv("BMI714_NHANES2020_Data.csv")

# the meta groups that exist within variables, I'm interested in laboratory
table(var_lib$Component)
```

### Dealing with NAs

```{r}
## first subset main dataframe to just lab variables
lab_var <- var_lib%>%
  filter(Component == "Laboratory")%>%
  pull(BMI_714_Variable_Name)

nhanes_lab <- nhanes_2020%>%
  select(all_of(lab_var))
#head(nhanes_lab)
```

Seems like there's a lot of NAs, let's see how bad it is

```{r}
# there exists no patient that did all 438 lab tests, so let's find a subset of lab vars for which we have a good size of patients 
# drop_na(nhanes_lab)

# first let's see how many NA's each variable has
n_patients <- nrow(nhanes_lab)


# I changed from using read.csv to fread for importing nhanes and the code below is not working can you help?
na_prop_list <- c()
for (i in 1:ncol(nhanes_lab)) {
  na_prop_list[i] <- sum(is.na(nhanes_lab[,i]))/n_patients
}
names(na_prop_list) <- colnames(nhanes_lab)

hist(na_prop_list)
# so a majority of the lab vars contain ~0.8 NA's

# let's set 0.5 as an NA threshold for subsetting lab variables, 
# with the hope that the leftover lab vars will yield a non-zero patient cohort
low_na_vars <- names(na_prop_list[na_prop_list<0.5])
length(low_na_vars)
nrow(nhanes_lab%>%select(all_of(low_na_vars))%>%drop_na())
```

We saw that each of the lab variables have differing rate of NAs. 

  *sanity check*: if a variable A has 0.5 NA rate, then that means half of the total 15560 patients will have an NA for that variable. Our goal is to find a subset of patients and a set of variables for which there exists no NAs. If we introduce some variable B also with 0.5 NA rate and we find the patients that have no NA for either variable, we should expect a result size of less than half of 15560 because it's not necessarily true that everyone who tested for var A also tested for var B (we can sure hope).
  
Thus, we have that 1) the lower the NA rate threshold, the less resulting variable subset, 2) the less resuling variable subset, the more patients will be leftover at the end. (consider the base case where only the lab var with NA rate of 0.00001 is left, which means almost all 15560 patients will be selected)

If we look at only the variables with at most 0.5 NA rate, we end up with ~5000 patients. Let's see if we can do better, our goal is to maximize the size of subsetted variables while having the most patients we can work with. 

```{r}
# set threshold value from 0.01 to 0.50, expect more consensus patient as threshold decrease
consensus_patient_n <- c()
filtered_var_n <- c()
i <- 1
for (t in seq(0.01, 0.5, by = 0.01)) {
  low_na_vars <- names(na_prop_list[na_prop_list<t])
  filtered_var_n[i] <- length(low_na_vars)
  consensus_patient_n[i] <- nrow(nhanes_lab%>%select(low_na_vars)%>%drop_na())
  i <- i + 1
}
names(consensus_patient_n) <- seq(0.01, 0.5, by = 0.01)
names(filtered_var_n) <- seq(0.01, 0.5, by = 0.01)

# aggregate results in dataframe for plotting
results_df <- data.frame(
  Threshold = seq(0.01, 0.5, by = 0.01),
  Consensus_Patients = consensus_patient_n,
  Filtered_Variables = filtered_var_n
)%>%filter(Filtered_Variables > 0)


# the lower the threshold the more patients
ggplot(results_df, aes(x = Threshold, y = Consensus_Patients)) +
  geom_line() +
  geom_point() +
  scale_x_reverse() + 
  labs(
    title = "Consensus Patients vs NA Threshold",
    x = "NA Threshold (Reversed)",
    y = "Number of Consensus Patients"
  ) +
  theme_classic()

# the lower the threshold the less variables
ggplot(results_df, aes(x = Threshold, y = Filtered_Variables)) +
  geom_line() +
  geom_point() +
  scale_x_reverse() + 
  labs(
    title = "Filtered Variables vs NA Threshold",
    x = "NA Threshold (Reversed)",
    y = "Number of Filtered Variables"
  ) +
  theme_classic()

# post hoc
label_point <- results_df[results_df$Threshold == 0.31, ]

min(results_df$Filtered_Variables)

# the more variable the less consensus patients 
# pdf(paste0(plot.out.dir, "/na_handling.pdf"), width = 5, height = 4)
ggplot(results_df, aes(x = Filtered_Variables, y = Consensus_Patients)) +
  geom_line() +
  geom_point() +
  geom_text_repel(
    data = label_point,
    aes(label = paste("NA rate =", Threshold, "\n Filtered Variables # =", Filtered_Variables, "\n Patient # =", Consensus_Patients)),
    nudge_x = 30,  # Nudges the label to the right
    nudge_y = 2000, # Nudges the label upwards
    segment.color = "blue", # Line color
    segment.size = 0.5,     # Line thickness
    color = "blue",         # Label text color
    box.padding = 0.5,      # Space around the text
    point.padding = 0.5     # Space around the point
  ) +
  labs(
    title = "Figure 1: Number of Consensus \nPatients vs Number of Filtered Variables",
    x = "Number of Filtered Variables",
    y = "Number of Consensus Patients"
  ) +
  theme_classic()
# dev.off()
```

By elbow rule, we pick NA rate to be 0.31, and continue with the leftover data subset.

### How do the selected lab variables correlate with one another?

We want to see what variables seem to group together, but also look at whether or not we see at least some correlation between our hypothesis group of variables (PBCD) and other groups of variables.

```{r}
selected_vars <- names(na_prop_list[na_prop_list<0.31])
nhane_sub <- nhanes_lab%>%select(selected_vars)%>%drop_na()
# summary(nhane_sub)

# what are these lab values actually corresponding to?
# table(var_lib%>%filter(BMI_714_Variable_Name %in% colnames(nhane_sub))%>%pull(Data_File_Description))

# large scale correlation check by heatmap
## but first drop two columns that have 0 standard deviation -- the values were probably all 0 anyways
# pdf(paste0(plot.out.dir, "/correlation_subsetted_labvar.pdf"), width = 25, height = 25)
nhane_sub <- nhane_sub[, apply(nhane_sub, 2, sd) != 0]
cor_matrix <- cor(nhane_sub, use = "complete.obs", method = "pearson")
sub_anno <- var_lib%>%
  filter(BMI_714_Variable_Name %in% colnames(nhane_sub))%>%
  select(BMI_714_Variable_Name, Data_File_Description)
cor_matrix_ordered <- cor_matrix[,sub_anno$BMI_714_Variable_Name]
p <- pheatmap(cor_matrix_ordered, 
         color = colorRampPalette(brewer.pal(n = 11, name = "Spectral"))(50),
         display_numbers = F, cluster_rows = F, cluster_cols = F)
p
# dev.off()
```
Seems like variables within "Complete Blood Count with 5-Part Differential in Whole Blood" and "Cotinine and Hydroxycotinine - Serum" have some strong covariance within the group. Across groups, we see 1) some correlation between some CBC tests and PBCD tests, 2) slight correlation between some PBCD items and HEPB, HEPC, HEPE measurements, 3) notable correlation between blood mercury (P_PBCD_LBXTHG, P_PBCD_LBDTHGSI, P_PBCD_LBDTHGLC) and mercury level in the IHGEM tests...(yeah no sh*t), 4) slight correlation between blood selenium, blood manganese levels and hemoglobin/red cell/platlet measurements

With that, we focus on observation 2), and aim to construct a regression model that can best predict Hepatitis B core antibody (P_HEPBD_LBXHBC) from "Lead, Cadmium, Total Mercury, Selenium, & Manganese - Blood (PBCD)" family of tests. Upon further inspection, we see that there is actually redundancy within the PBCD tests. For example, the three variables P_PBCD_LBXTHG, P_PBCD_LBDTHGSI, and P_PBCD_LBDTHGLC all measure blood mercury, the first in ug/L, second in nmol/L, and the third a code that defines whether or not the level is above some threshold level. We decide to keep only the variables that uses nmol/L for the 5 heavy metal tests because it's the conventional unit used in research papers. To increase the scope of the variables arrive at 8 variables, we additionally include hematocrit (P_CBC_LBXHCT), mean cell volume (P_CBC_LBXMCVSI), and hemoglobin (P_CBC_LBXHGB). 

```{r}
# creating the dataset with the selected variables
# hep_vars <- intersect(var_lib$BMI_714_Variable_Name[grep("P_HEP", var_lib$Data_File_Name)], colnames(nhane_sub))
hep_vars <- "P_HEPBD_LBXHBC"
analysis_ind_var_vec <- c("P_PBCD_LBDBPBSI", "P_PBCD_LBDBCDSI", "P_PBCD_LBDTHGSI", "P_PBCD_LBDBSESI", "P_PBCD_LBDBMNSI", "P_CBC_LBXHCT", "P_CBC_LBXMCVSI", "P_CBC_LBXHGB")
analysis_var_ref <- var_lib%>%filter(BMI_714_Variable_Name %in% c(hep_vars, analysis_ind_var_vec))
# subset the data to only include the variables of interest
nhane_selected <- nhane_sub%>%
  select(all_of(c(hep_vars, analysis_ind_var_vec)))

# we see that LBXHBC is indeed binary
table(nhane_selected$P_HEPBD_LBXHBC)
nhane_selected$P_HEPBD_LBXHBC <- as.factor(ifelse(nhane_selected$P_HEPBD_LBXHBC == 1, 0, 1))

head(nhane_selected)

cor(nhane_selected[, -1])
plot(nhane_selected[, -1])
# table(nhane_selected$P_HEPBD_LBXHBC)
```
We see an almost 1 correlation between P_CBC_LBXHGB and P_CBC_LBXHCT, which is expected as they are both blood cell measurements. We will drop P_CBC_LBXHGB from the analysis.

```{r}
nhane_selected <- nhane_selected%>%
  select(-c(P_CBC_LBXHGB))
analysis_ind_var_vec <- analysis_ind_var_vec[analysis_ind_var_vec != "P_CBC_LBXHGB"]
```

### Scaling

```{r}
nhane_selected <- nhane_selected %>%
  mutate(across(!matches("P_HEPBD_LBXHBC"), ~ (. - mean(.)) / sd(.)))
```

### Test-train split

```{r}
set.seed(714)
# since we have so many observations we can do 0.7 train prop (i'm so happy)
train_prop <- 0.7

n <- nrow(nhane_selected)

# train_indices <- sample(1:n, size = floor(train_prop * n))
# using this protocol to ensure equal ratio of case/control in train/test
train_indices <- createDataPartition(nhane_selected$P_HEPBD_LBXHBC, p = 0.7, list = FALSE)

nhane_train <- nhane_selected[train_indices, ]   # Training set
nhane_test <- nhane_selected[-train_indices, ]  # Test set

# Check the sizes
cat("Training set size:", nrow(nhane_train), "\n")
cat("Test set size:", nrow(nhane_test), "\n")

cat("Class proportions in training set:\n")
print(prop.table(table(nhane_train$P_HEPBD_LBXHBC)))

cat("Class proportions in test set:\n")
print(prop.table(table(nhane_test$P_HEPBD_LBXHBC)))
```


## Model Building

### Logistic Regression

We have 7 continuous independent variables and 1 binary dependent variable. We will use logistic regression to predict the probability of having Hepatitis B core antibody based on the 8 blood test results. I am interested in investigating the performance and analyzing different approaches to feature selection. 

#### Approach 1: Backward Elimination with $\\beta_1$ significance level of 0.05

```{r}
# fit a full model
fit_full <- glm(P_HEPBD_LBXHBC ~ ., data = nhane_train, family = binomial)
summary(fit_full)   #P_PBCD_LBDBMNSI is not significant

fit1_1 <- update(fit_full, . ~ . - P_CBC_LBXHCT)
summary(fit1_1) #P_PBCD_LBDBSESI is not significant

fit1_2 <- update(fit1_1, . ~ . - P_PBCD_LBDBSESI)
summary(fit1_2)

fit1 <- update(fit1_2, . ~ . - P_PBCD_LBDBMNSI)
summary(fit1)
# all variables are p<0.05, we are done now
```

#### Approach 2: Backward Elimination with AIC

```{r}
fit_null <- glm(P_HEPBD_LBXHBC ~ 1, data = nhane_train, family = binomial)
fit2 <- step(fit_full, scope = list(lower = fit_null, upper = fit_full), data = nhane_train, direction = "backward", trace = F, k = 2)
summary(fit2)
```

#### Approach 3: Iterative Evaluation

Method taken from Aparna, pseudo code below:

Let the set of predictor variables be $V$, 
Construct a null model $M_0$
Let $M_curr$ be $M_0$
while $V$ is not empty
  for each $v \in V$
    fit a univariate model $M_{v}$ with $v$
    run F test to compare $M_{v}$ to $M_{curr}$
    record $v$ and the p value of the F test ($p_v$) for $M_v$ in a table
    
  if all $p_v$ > 0.05, stop
  else
    identify $v$ with $min(p_v)$
    $M_curr$=$M_v$
    $V$ = $V$ - $v$

```{r}
# initialize model and variable set
curr_ind_var_set <- analysis_ind_var_vec
fit_iter <- fit_null
while (length(curr_ind_var_set) > 0) {
  # initiatlize var stat table
  var_sig <- data.frame()
  for (ind_var in curr_ind_var_set) {
    # create a formula with +1 variable
    formula_in <- as.formula(paste(paste(deparse(formula(fit_iter)), collapse = ""), "+", ind_var))
    
    # fit the model
    fit <- glm(formula_in, data = nhane_train, family = binomial)
    
    # pulls out p value of this F-test
    curr_p <- anova(fit_iter, fit)[[5]][2]
    
    # store the variable, p value, and AIC
    var_sig <- rbind(var_sig, data.frame(var = ind_var, p_value = curr_p))
  }
  
  if (all(var_sig$p_value > 0.05)) {
    # if all p values are > 0.05, stop
    fit3 <- fit_iter
    cat("Finished iterative variable selection, check below for the summary of final model\n")
    break
  }
  
  # best var by p value
  best_var_p <- var_sig[var_sig$p_value == min(var_sig$p_value),]$var
  fit_iter <- update(fit_iter, as.formula(paste(". ~ . +", best_var_p)))
  curr_ind_var_set <- curr_ind_var_set[curr_ind_var_set != best_var_p]
  
  cat(paste(best_var_p, "added to the model\n"))
}

summary(fit3)
```

#### Quick comparison before moving forward

Now that we have the three models, let's compare

```{r}
#Backward Elimination with $\\beta_1$ significance level of 0.05
summary(fit1)
#Backward elimination with AIC
summary(fit2)
#Iterative evaluation
summary(fit3)
```

These three models agree with each other perfectly. We will now proceed with fit1. Just for reference, AIC for fit1 is 2944. And a quick comparison between fit1 and fit_full (model with all 7 predictors) with r squared value indeed shows that our variable selection made a significant improvement in model fit.

```{r}
AIC(fit3,fit2, fit1)
fit_logi <- fit1# fit1 is the best model

rsq(fit1, adj = T)
rsq(fit_full, adj = T)
```

Out of curiosity, what if we now add back in P_CBC_LBXHCT? Would we see any significant improvement in model fit? 

```{r}
fit_blah <- update(fit1, . ~ . + P_CBC_LBXHCT)
summary(fit_blah)

AIC(fit_blah, fit1)
anova(fit1, fit_blah)
```
Ok that's comforting. Indeed, HCT does not add any significant improvement to the model.

#### Ridge Regression

We will now perform ridge regression on the selected variables to see if we can improve the model and prevent overfitting. We will examine whether or not ridge indeed lives up to its promise. We will use cross-validation to find the best lambda value.

```{r}
# Ridge regularization because want to retain all covariates
# remove the variables that were dropped during variable selection
nhane_train_ridge <- nhane_train%>%
  select(-c(P_PBCD_LBDBSESI, P_PBCD_LBDBMNSI, P_CBC_LBXHCT))
nhane_test_ridge <- nhane_test%>%
  select(-c(P_PBCD_LBDBSESI, P_PBCD_LBDBMNSI, P_CBC_LBXHCT))

nhanes_matrix <- apply(nhane_train_ridge, 2, as.numeric)

#set alpha to 0 for ridge
set.seed(714)
nhanes_model_ridge <- cv.glmnet(nhanes_matrix[,!colnames(nhanes_matrix) %in% "P_HEPBD_LBXHBC"], nhanes_matrix[,"P_HEPBD_LBXHBC"], nfolds = 10, alpha = 0)
best_lambda <- nhanes_model_ridge$lambda.min
fit_ridge <- glmnet(nhanes_matrix[,!colnames(nhanes_matrix) %in% "P_HEPBD_LBXHBC"], nhanes_matrix[,"P_HEPBD_LBXHBC"], alpha = 0, lambda=best_lambda)
ridge_coefficients<- coef(fit_ridge)
ridge_coefficients_vector <- as.vector(ridge_coefficients)
names(ridge_coefficients_vector) <- rownames(ridge_coefficients)
```

We now compare the coefficients from the ridge regularization to the coefficients of fit_out.

```{r}
comparison_df <- data.frame( 
  Ridge = ridge_coefficients_vector,  
  Standard = fit_logi$coefficients[names(ridge_coefficients_vector)] 
)
comparison_df
```

```{r}
# model eval with regular logistic regression
predicted_probs <- predict(fit_logi, newdata = nhane_test, type = "response")
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
# table(predicted_classes)
actual_classes <- nhane_test[, "P_HEPBD_LBXHBC"]
# table(actual_classes)
confusion_matrix_logi <- table(Predicted = predicted_classes, Actual = actual_classes)
roc_logi <- roc(actual_classes, predicted_probs, plot = T, print.auc = T, main = "ROC Curve for Logistic Regression")

# model eval with ridge
predicted_probs <- predict(fit_ridge, newx = as.matrix(nhane_test_ridge[, !colnames(nhane_test_ridge) %in% "P_HEPBD_LBXHBC"]), s = "lambda.min", type = "response")
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
# table(predicted_classes)
actual_classes <- nhane_test_ridge[, "P_HEPBD_LBXHBC"]
# table(actual_classes)
confusion_matrix_ridge <- table(Predicted = predicted_classes, Actual = actual_classes)
roc_logi_ridge <- roc(actual_classes, predicted_probs, plot = T, print.auc = T, main = "ROC Curve for Ridge Regression")
```
We now obtain the overview of metrics for the two models.

```{r}
# define a function to get metrics from confusion matrix
calculate_metrics <- function(confusion_matrix) {
  TN <- confusion_matrix[1, 1]
  FP <- confusion_matrix[1, 2]
  FN <- confusion_matrix[2, 1]
  TP <- confusion_matrix[2, 2]
  sensitivity <- TP / (TP + FN)
  specificity <- TN / (TN + FP)

  # return a quick list of metrics
  return(list(
    Sensitivity = sensitivity,
    Specificity = specificity,
    False_Negatives = FN,
    False_Positives = FP
  ))
}
```

```{r}
# perform the functions on the confusion matrix
metrics_logistic <- calculate_metrics(confusion_matrix_logi)
metrics_ridge <- calculate_metrics(confusion_matrix_ridge)

# make a df
evaluation_df <- data.frame(
  Model = c("Logistic Regression", "Ridge Regression"),
  Sensitivity = c(metrics_logistic$Sensitivity, metrics_ridge$Sensitivity),
  Specificity = c(metrics_logistic$Specificity, metrics_ridge$Specificity),
  False_Negatives = c(metrics_logistic$False_Negatives, metrics_ridge$False_Negatives),
  False_Positives = c(metrics_logistic$False_Positives, metrics_ridge$False_Positives)
)

evaluation_df

```
## Model Diagnostics

### Bootstrapping for CI

First define a function that takes in a model and outputs coefficients as required by boot. This performs 1000 bootstrapping to generate a distribution (and therefore 95 CI) for each coefficient. We use the training set instead of the whole dataset because the model that we want to diagnose is built on training data so bootstrapping using it mirrors this process and helps validate its stability. 

```{r}
# create the data pool for bootstrapping
bs_dat <- nhane_train%>%
  select(-c(P_PBCD_LBDBSESI, P_PBCD_LBDBMNSI, P_CBC_LBXHCT))

x <- model.matrix(P_HEPBD_LBXHBC ~ ., data = bs_dat)[, -1]
y <- as.numeric(bs_dat$P_HEPBD_LBXHBC) - 1

pb <- progress_bar$new(total = 1001)
# ridge regression bootstrapping function
ridge_boot_fn <- function(in_dat, idx) {
  pb$tick()  
  # get the bootstrap sample
  x_boot <- in_dat[idx, 2:5]
  y_boot <- in_dat[idx, 1]
  
  # edge case where y is all the same
  if (length(unique(y_boot)) == 1) {
    cat("all y are same level")
    return(rep(NA, ncol(x_boot) + 1))  # Return NA coefficients for this sample
  }
  
  # fit ridge, I know finding best lambda for each boot is time consuming
  # it's not too bad (~1 min) so we'll go for that accuracy
  pre_cv <- cv.glmnet(x_boot, y_boot, nfolds = 10, alpha = 0)
  best_lambda <- pre_cv$lambda.min
  fit_ridge <- glmnet(x_boot, y_boot, alpha = 0, lambda=best_lambda)
  
  # get coefficients
  return(as.numeric(coef(fit_ridge)))
}


# perform boot
data_list <- list(x = x, y = y)
set.seed(714)
ridge_boot_results <- boot(
  data = cbind(y,x), 
  statistic = ridge_boot_fn, 
  R = 1000
)

ridge_boot_results
```

### Permutation to Check Calibration

Now let's do 1000 permutation and look at the null distribution of the coefficients and the distribution p values. Again, we use training set as the base to ensure consistency. 

```{r}
x <- nhanes_matrix[,!colnames(nhanes_matrix) %in% "P_HEPBD_LBXHBC"]
y <- nhanes_matrix[,"P_HEPBD_LBXHBC"]

set.seed(714)
# first calculate original model performance, note this is not on test set
original_model <- cv.glmnet(x, y, alpha = 0, nfolds = 10)
original_lambda <- original_model$lambda.min
original_fit <- glmnet(x, y, alpha = 0, lambda = original_lambda)
coef(fit_ridge)

original_coefs <- as.numeric(coef(original_fit))  # Include intercept
coef_names <- rownames(coef(original_fit))

# we now calculate performance for each permutation
set.seed(714)
n_perm <- 1000
perm_coefs <- matrix(NA, nrow = n_perm, ncol = length(original_coefs))
perm_p_val <- matrix(NA, nrow = n_perm, ncol = length(original_coefs))

pb <- progress_bar$new(total = 1001)
for (i in 1:n_perm) {
  pb$tick()
  # Permute y
  y_permuted <- sample(y)
  
  # ridge for coefficient
  permuted_model <- cv.glmnet(x, y_permuted, alpha = 0, nfolds = 10)
  permuted_lambda <- permuted_model$lambda.min
  permuted_fit <- glmnet(x, y_permuted, alpha = 0, lambda = permuted_lambda)
  perm_coefs[i, ] <- as.numeric(coef(permuted_fit))
  
  # regular glm for p value
  glm_fit <- glm(y_permuted ~ x, family = "binomial")
  
  glm_summary <- summary(glm_fit)
  perm_p_val[i, ] <- as.numeric(glm_summary$coefficients[, 4])
}
coef_names <- c("(Intercept)", colnames(x))

# coefficient tbl and histogram
coef_value_tbl <- data.frame(Permutation = 1:n_perm, perm_coefs)
colnames(coef_value_tbl) <- c("Permutation", coef_names)
for (i in 1:length(coef_names)) {
  hist(
    coef_value_tbl[, i], 
    main = paste("Distribution of Coefficient for ", coef_names[i], " Under Permutation"), 
    xlab = "Coefficient Value", 
    col = "lightblue"
  )
}

# p value tbl and histogram
p_value_tbl <- data.frame(Permutation = 1:n_perm, perm_p_val)
colnames(p_value_tbl) <- c("Permutation", coef_names)
for (i in 1:length(coef_names)) {
  hist(
    p_value_tbl[, i], 
    main = paste("Distribution of P Value for ", coef_names[i], " Under Permutation"), 
    xlab = "P Value", 
    col = "lightblue"
  )
}
```